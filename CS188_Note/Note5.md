# Game 博弈

在第一篇Note中，我们讨论了搜索问题以及如何高效且最优地解决他们 —— 使用强大的通用搜索算法，我们的智能体可以确定最佳的可能计划，然后简单地执行它以达到目标。

现在，让我们换个角度来考虑这样的场景：智能体有一个或多个试图阻止它们达到目标的**对手**（adversaries）。智能体不能再运行我们已经学过的搜索算法来制定计划，因为我们*通常不能确切地知晓对手会如何计划对付我们以及如何对我们的行动做出反应*。

相反，我们需要运行一种新的算法来生成**对抗搜索问题**（adversarial search problem）的解，这种问题更常被称为**博弈**（games）

博弈的类型有很多，其可以有产生确定性或随机（概率性）结果的行动，可以有任意数量的玩家，也可以是**零和**的（zero-sum）

> 在某些情况下，行动产生随机结果意味着我们不能完全观测到行动产生的效果，这可能是因为对环境的不完全观测。
>
> “零和” 意味着对一方有利的东西将对另一方同等程度有害：不存在 “双赢” 结果（二者之和为0）
>
> 在博弈论中，我们通常用**移动**（move）作为 “动作”（action）的同义词，用**局面**（position）作为 “状态”（state）的同义词。

我们将讨论的第一种博弈类型是**确定性零和博弈**（deterministic zero-sum games），这种博弈的行为具有确定性并且我们的收获直接等价于对手的损失，反之亦然。

考虑这种博弈最简单的方法是将其定义为一个单一的变量值。一个团队或智能体试图最大化它，而另一个团队或智能体试图最小化它，从而有效地将他们置于直接的竞争中。

在吃豆人中，这个变量是分数。你试图通过快速且高效地吃小球来最大化分数，而鬼魂通过先吃掉你来最小化分数。

许多普通的家喻户晓的博弈也属于这一类：

- **跳棋**（Checkers）：第一个跳棋计算机玩家于1950年问世。此后，跳棋成为一个已解游戏（**solved game**），这意味着在任意一方采取最优行动的前提下，任何局面都能够以确定性方式被评估为胜利、失败或平局。

- **国际象棋**（Chess）：1997年，“深蓝” 成为第一个在六场比赛中击败人类国际象棋冠军 Gray Kasparov 的计算机智能体。“深蓝” 由极其复杂的方法构建而成，每秒可以评估超过两亿个局面。目前的项目甚至性能更好，尽管没有那么具有历史意义。

- **围棋**（Go）：围棋的搜索空间比国际象棋大得多，因此大多数人不相信围棋计算机智能体能在未来几年内击败人类世界冠军。然而，由Google开发的AlphaGo在2016年3月历史性地击败了围棋冠军 Lee Sodol

<center><img src="./pic/comparison_chart.png" width=350></center>

以上所有世界冠军智能体至少在某种程度上都使用了我们将要介绍的**对抗搜索技术**（adversarial search technique）

与返回完整计划的常规搜索不同，对抗搜索返回一个策略或政策（a **strategy** or **policy**），它只是在给定智能体和他们的对手的一些配置的情况下，推荐最好的可能移动。

> 由于竞争的存在，且不知道对手的决策方法，我们无法准确的预测出一条完整的路径。虽然我们可以以一定的概率模拟对手可能进行的移动，但是对于一次确定的博弈，对手在某次移动时的方式是确定的，这种预测对于实际过程没有指导意义。
>
> 相反，只是给出接下来应该做的移动，并在下一次根据这些移动与环境交互的反馈来进行下一步的决策，可以更好的应对敌方的行动。

我们很快就会看到，这种算法具有通过计算产生行为的美丽特性 —— 我们运行的计算在概念上相对简单、可广泛推广，且天然就能在同一团队的智能体之间产生合作并 “超越” 对手。

标准的**博弈形式**包括以下定义：

- **初始状态**：$ s_0 $

- **参与者**：$ Players(s) $ 返回状态 $ s $ 下，轮到其移动的参与者

- **移动**：$ Actions(s) $ 返回状态 $ s $ 下，参与者的所有合法移动

- **转移模型**：$ Result(s, a) $ 返回状态 $ s $ 下，执行动作 $ a $ 所产生的结果结果状态

- **终止测试**（Terminal test）：$ Terminal-test(s) $ 测试状态 $ s $ 是否为终止状态

- **终止值**（Terminal values）：$ Utility(s, player) $ 效用函数，返回博弈结束时终止状态 $ s $ 下参与者 $ player $ 最终的数值收益

## Minimax 极小化极大

我们将考察的第一个零和博弈算法是**极小化极大**（Minimax），该算法在以下激励假设（motivating assumption）下运行：*我们所面对的对手会表现出最优的行为，并且总执行对我们最不利的移动*。

为了介绍该算法，我们必须先形式化**终止效用**（terminal utilities）和**状态值**（state value）的概念。一个状态的值是处于该状态的智能体所能获得的最优分数。为直观理解这个概念的意义，观察以下平凡且简单的吃豆人游戏棋盘（board - 将地图网格化，近似认为是一个棋盘）：

<center><img src="./pic/Pacman_game_board.png" width=180></center>

假设开始时吃豆人有10分，它每移动一次会损失1分，直到它吃到小球，此时博弈到达终止状态并结束。

我们可以为这个棋盘构建**博弈树**（game tree），其中状态的子节点和普通搜索算法的搜索树一样代表后继状态，如下所示：

<center><img src="./pic/Pacman_game_tree.png" width=500></center><br>

从树中可以清楚看到，如果吃豆人直接走向小球，它在博弈结束时将获得8分。而如果它在任何一点后退，它在博弈结束时获得的分数将更低。既然我们已经生成了一个有若干终止和中间状态的博弈树，我们就可以形式化这些状态的值的含义了。

**状态的值**被定义为智能体从该状态能取得的最佳可能结果（效用）。稍后我们会更具体的形式化效用（utility）的概念，但现在简单的将智能体的效用看作它获得的分数或点数就足够了。终止状态的值称为**终止效用**（terminal utility），其总是一些确定的已知值和固有的博弈属性。

> 在博弈中，终止状态的值总是可以通过一种固定的算法或者规则来确定，正如围棋或象棋中那样。且给出一个终止状态总是可以将其唯一地映射到某个实数值。所以说它是一个固有的博弈属性。

在吃豆人例子中，最右端的终止状态的值就是8，这是吃豆人直接走向小球所或得的分数。此外，在本例中，非终止状态的值被定义为其子状态的值的最大值。定义 $ V(s) $ 为状态 $ s $ 的值函数，上述讨论可总结如下：

$$
\begin{align}
    \forall \text{ non-terminal states},\space & V(s) = \max_{s' \in successors(s)} V(s') \tag{1} \\
    \forall \text{ terminal states},\space & V(s) = \text{known} \tag{2} \\
\end{align}
$$

这建立了一个非常简单的递归规则。利用该规则不难得出，根节点的直接右子节点的值应为8，直接左子节点的值应为6，因为这分别是智能体从起始状态向右或向左移动所能获得的最大可能分数。

> 注意：这里所说的递归并不是说状态的值等于所有子孙节点对应的状态值的最大值，而是说由于我们并没有计算出后继节点的值，所以需要递归地求解出后继节点的值以用于更新当前节点的状态值。

因此，通过运行这样的计算，智能体可以确定向右移动是最优的，因为开始状态的右子状态的值大于左子状态的值。

现在让我们引入一个新的博弈棋盘，其内有想阻止吃豆人吃掉小球的敌对幽灵。

<center><img src="./pic/Pacman_game_board_2.png" width=180></center>

**游戏规则**规定两个智能体轮流移动，这导致在博弈树的各层中两个智能体交替进行控制。智能体控制一个节点意味着该节点对应一个状态，在这个状态下轮到了该智能体行动，因此这是他们决定采取行动并相应地改变游戏状态的机会。

这是由上述新的双智能体博弈棋盘产生的博弈树：

<center><img src="./pic/two_agent_game_tree.png" width=500></center>

蓝色节点由吃豆人控制并决定采取什么行动，而红色节点是鬼魂控制的节点。注意：鬼魂控制的节点的所有子节点都是鬼魂从父节点状态向左或向右移动的节点，对于吃豆人控制的节点而言也是这样。

简单起见，让我们将这个博弈树截断为一棵深度为2的树，并且给终止状态赋予一个*虚构的值*（spoofed value），如下所示：

<center><img src="./pic/two_agent_game_tree_depth2.png" width=500></center>

自然地，引入鬼魂控制的节点改变了吃豆人认为的最优移动方式。新的最优移动可以用极小化极大算法确定。

不同于在树的每一层上都最大化子节点的效用，极小化极大算法仅在由 Pacman 控制的节点的子节点上最大化，而在由幽灵控制的节点的子节点上最小化。因此，上图中的两个鬼魂节点的值分别为 $ \min(-8, -5) = -8 \text{ and } \min(-10, +8) = -10 $。相应地，吃豆人控制的根节点的值为 $ \max(-8, -10) = -8 $。

> 由于节点的值对应的是吃豆人所能获得的分数，因此吃豆人想要最大化它，而鬼魂想要最小化。

因为吃豆人想要最大化他的分数，所以他会走左边并得到-8分，而不是试图走向小球并获得-10分。这是通过计算指导行为的最好例子：尽管吃豆人希望得到+8分，如果他最终到达最右边的子状态，通过 minimax 算法他 “知道” 一个表现最佳的鬼魂不会让他到达。

为了采取最优行动，吃豆人被迫对冲赌注，并与直觉相反地远离小球，以最小化他失败的程度。

> 对冲赌注意味着吃豆人选择获取一个相对较小的分数，并避免可能的更差收益。

minimax 给状态赋值的方法如下：

$$
\begin{align}
    \forall \text{ agent-controlled states, } & V(s) = \max_{s'\in successors(s)} V(s') \tag{1} \\
    \forall \text{ opponent-controlled states, } & V(s) = \min_{s' \in successors(s)} V(s') \tag{2} \\
    \forall \text{ terminal states, } & V(s) = \text{ known} \tag{3} \\
\end{align}
$$

具体实现上，minimax 的行为类似于 DFS，其以与DFS相同的顺序计算节点的值。从最左边的终止节点开始，迭代的向右遍历计算。更准确地说，其在博弈树上执行**后序遍历**。

得到的 minimax 伪代码既优雅有直观简单，如下所示。注意：minimax 将返回一个移动，该移动对应于根节点到它获取值的子节点的分支。

<center><img src="./pic/minimax_value_pseudocode.png" width=540></center><br>

> 由于我们不知道对手采取行动的策略，所以我们无法直接获得一个行动序列，而是仅仅获得一个行动，然后根据对手的行动进行动态的应对。
>
> 但这并不意味着我们不需要穷举所有可能情况，因为算法核心步骤 —— 赋值的定义中明确指出需要在所有后继节点中选取最大 / 最小值。
>
> 不过需要指出的是，我们只需要遍历一遍求值，然后每次行动时查找已计算出的结果即可。
